---
layout: default
title: Welcome!
---

**Hi, I'm Louis!** I'm a technical AI safety researcher and recent Oxford MCompSciPhil graduate from Hertford College.

------------------------

## About me

- In my research, I'm primarily interested in *AI control protocols*, *chain-of-thought monitoring*, and *scheming evals*.
  - My previous work includes elements of game theory, multi-agent risks, corrigibility, and active learning.

- I'm also a committee member of *OAISI*, the [Oxford AI Safety Initiative](https://oaisi.org); my priority is to build a welcoming, kind, and collaborative AI safety community here in Oxford.
  - [Please reach out](https://savvycal.com/oaisi/louis) if you want to talk about what AI safety is / what you might want to work on / how OAISI might be able to help you.

- Aside from academic work, I'm pretty musical: I play bass in a couple bands, noodle on guitar and keys when writing my own music, and secretly would love to make a career out of playing music. Alternate career paths for me would also include teaching and doing outreach!

- Some of my loves include \[video/board\] games and puzzles of all kinds, Japanese food (part of my heritage), continental philosophy, Arsenal FC (COYG), and most of all my lovely fianc√©e :\)

Shoot me an email if you want to talk - I'm quick to respond and I enjoy talking to new people! Get in touch via [louis@cbthomson.com](mailto:louis@cbthomson.com)

------------------------

## My work

*... in order of recency...*

- **Agentic Monitoring for AI Control** *\[2025\]* (supervised by [Tyler Tracy](https://www.tylertracy.com/))
  - *An initial investigation into the extent to which trusted monitors benefit from opportunities to be agentic.* See my [blog post](https://www.lesswrong.com/posts/ptSXTkjnyj7KxNfMz/agentic-monitoring-for-ai-control-1) for an introduction to the research direction alongside some initial results and discussion.

- **Cooperation and Control in Markov Delegation Games** *\[2025\]* (supervised by [Lewis Hammond](https://lewishammond.com/) and [Oly Sourbut](https://uk.linkedin.com/in/oliver-sourbut))
  - *Formalising these two key dimensions along which multi-agent delegation games can produce bad outcomes for humans.* This was carried out as part of my Master's year at Oxford; see my [report](https://drive.google.com/file/d/1EJS2ljeih-qq6OH2abr0GwTM75guNaX1/view?usp=sharing). \[Note: I'd be keen to finish this work some day and turn it into a workshop paper!\]

- **Model Models: Simulating a Trusted Monitor** *\[2025\]*
  - *Can an untrusted model predict how a trusted monitor will score its solutions?* This was part of the Apart Research [AI Control Hackathon](https://apartresearch.com/sprints/ai-control-hackathon-2025-03-29-to-2025-03-30) in March 2025; see the [project page](https://apartresearch.com/project/model-models-simulating-a-trusted-monitor-r682) containing a report and the codebase.

- **Games for AI Control** *\[2024-5\]* (in collaboration with [Charlie Griffin](https://www.cs.ox.ac.uk/people/charlie.griffin/); supervised by [Alessandro Abate](https://www.cs.ox.ac.uk/people/alessandro.abate/) and [Buck Shlegeris](https://www.shlegeris.com/))
  - *Introducing a game-theoretic model for AI Control settings.* See the [paper](https://arxiv.org/abs/2409.07985) and [blog post](https://www.lesswrong.com/posts/Yu8jADLfptjPsR58E/games-for-ai-control-1).

- **Towards shutdownable agents via stochastic choice** *\[2024\]* (supervised by [Elliott Thornley](https://www.elliott-thornley.com/))
  - *Working on a proposal to solve the corrigibility problem by training agents to have incomplete preferences.* I briefly worked on this project through the [Future Impact Group](https://futureimpact.group/fellowship) programme; you can see the resulting [paper](https://arxiv.org/abs/2407.00805) which was accepted to [TAIS 2025](https://www.tais2025.cc/proceedings/thornley-towards-shutdownable-agents).

- **Tall Tales at Different Scales: Evaluating Scaling Trends For Deception In Language Models** *\[2023\]* (supervised by [Francis Rhys Ward](https://francisrhysward.wordpress.com/))
  - *Investigating the extent to which belief consistency and deceptive behaviour scale with model size*. I began working on this project through the AI Safety Hub Labs programme (now [LASR Labs](https://www.lasrlabs.org/)). See our [blog post](https://www.alignmentforum.org/posts/pip63HtEAxHGfSEGk/tall-tales-at-different-scales-evaluating-scaling-trends-for) and a [follow-up paper](https://arxiv.org/abs/2410.04272).

------------------------
